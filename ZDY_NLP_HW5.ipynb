{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5924f076",
   "metadata": {},
   "source": [
    "### Тема «POS-tagger и NER»\n",
    "\n",
    "#### Задание. Написать теггер на данных с русским языком\n",
    "\n",
    "1) Проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
    "2) Написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "3) Сравнить все реализованные методы, сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a24bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pyconll\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4223d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train-c.conllu')\n",
    "test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb7988",
   "metadata": {},
   "source": [
    "Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e5852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for sent in train[:]:\n",
    "    train_data.append([(token.form, token.upos) for token in sent])\n",
    "\n",
    "test_data = []\n",
    "for sent in test[:]:\n",
    "    test_data.append([(token.form, token.upos) for token in sent])\n",
    "\n",
    "test_sents=[]      \n",
    "for sent in test[:]:\n",
    "    test_sents.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79929656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_tagger.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efca07e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8013933198775962"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(train_data)\n",
    "unigram_tagger.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c1d7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8057946480890683"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
    "bigram_tagger.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d68377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8043687740087245"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(train_data, backoff=bigram_tagger)\n",
    "trigram_tagger.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45b6a14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8046292076307051"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NN') \n",
    "tag = backoff_tagger(train_data,  \n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef38a6",
   "metadata": {},
   "source": [
    "Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6664ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "\n",
    "for sent in train_data[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append('NO_TOKEN' if tok[0] is None else tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "\n",
    "for sent in test_data[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append('NO_TOKEN' if tok[0] is None else tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d46bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf80a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(Vectorizer, Regression, ngram=(1, 5), analyzer='char'):\n",
    "    vectorizer = Vectorizer(ngram_range=ngram, analyzer=analyzer)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train_tok)\n",
    "    X_test = vectorizer.transform(test_tok)\n",
    "\n",
    "    r = Regression\n",
    "    r.fit(X_train, train_enc_labels)\n",
    "    \n",
    "    pred = r.predict(X_test)\n",
    "      \n",
    "    accuracy = accuracy_score(test_enc_labels, pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9013b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_vectorizer_ngram15 = calculate_accuracy(HashingVectorizer, LogisticRegression(random_state=0), ngram=(1, 5))\n",
    "hashing_vectorizer_ngram25 = calculate_accuracy(HashingVectorizer, LogisticRegression(random_state=0), ngram=(2, 5))\n",
    "count_vectorizer = calculate_accuracy(CountVectorizer, LogisticRegression(random_state=0), ngram=(1, 5))\n",
    "tfidf_vectorizer = calculate_accuracy(TfidfVectorizer, LogisticRegression(random_state=0), ngram=(1, 5))\n",
    "count_vectorizer_word = calculate_accuracy(CountVectorizer, LogisticRegression(random_state=0), ngram=(1, 2), analyzer='word')\n",
    "tfidf_vectorizer_word = calculate_accuracy(TfidfVectorizer, LogisticRegression(random_state=0), ngram=(1, 2), analyzer='word')\n",
    "count_vectorizer_xgb = calculate_accuracy(CountVectorizer, xgb.XGBClassifier(verbosity=0), ngram=(1, 5))\n",
    "tfidf_vectorizer_xgb = calculate_accuracy(TfidfVectorizer, xgb.XGBClassifier(verbosity=0), ngram=(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f8e93",
   "metadata": {},
   "source": [
    "Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9196f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>count_vectorizer</td>\n",
       "      <td>0.931213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_vectorizer</td>\n",
       "      <td>0.919155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tfidf_vectorizer_xgb</td>\n",
       "      <td>0.911941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hashing_vectorizer_ngram15</td>\n",
       "      <td>0.909603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>count_vectorizer_xgb</td>\n",
       "      <td>0.906270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hashing_vectorizer_ngram25</td>\n",
       "      <td>0.825822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tfidf_vectorizer_word</td>\n",
       "      <td>0.612690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>count_vectorizer_word</td>\n",
       "      <td>0.608340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Vectorizer  Accuracy\n",
       "2            count_vectorizer  0.931213\n",
       "3            tfidf_vectorizer  0.919155\n",
       "7        tfidf_vectorizer_xgb  0.911941\n",
       "0  hashing_vectorizer_ngram15  0.909603\n",
       "6        count_vectorizer_xgb  0.906270\n",
       "1  hashing_vectorizer_ngram25  0.825822\n",
       "5       tfidf_vectorizer_word  0.612690\n",
       "4       count_vectorizer_word  0.608340"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizers = ['hashing_vectorizer_ngram15', 'hashing_vectorizer_ngram25', 'count_vectorizer', 'tfidf_vectorizer', \n",
    "               'count_vectorizer_word', 'tfidf_vectorizer_word', 'count_vectorizer_xgb', 'tfidf_vectorizer_xgb']\n",
    "accuracy_scores = [hashing_vectorizer_ngram15, hashing_vectorizer_ngram25, count_vectorizer, tfidf_vectorizer, \n",
    "                   count_vectorizer_word, tfidf_vectorizer_word, count_vectorizer_xgb, tfidf_vectorizer_xgb]\n",
    "\n",
    "result = pd.DataFrame({'Vectorizer': vectorizers, \n",
    "                       'Accuracy': accuracy_scores})\n",
    "\n",
    "result.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e879c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
